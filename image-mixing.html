<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Image Mixing - MergeMix</title>
  <link rel="stylesheet" href="css/style.css" />
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <!-- Academic Icons for arXiv -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <!-- MathJax for mathematical formulas -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
</head>
<body>
  <div class="container">
    <header class="site-header">
      <div class="title-block">
        <h1 class="site-title">MergeMix for Image Classification Scenario</h1>
        <p class="subtitle">MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding</p>
      </div>
    </header>

    <main>
      <section class="panel fade-up">
        <h2 style="color: #dc2626; font-weight: bold;">Image Mixing via Token Merge</h2>
        <div class="implementation-step">
          <h4 style="color: #2563eb;">Step 1: Image Policy with Token Merging</h4>
          <p>
            We utilize a ViT model $f_\theta(\cdot)$ where $N$ attention layers are replaced with <strong>ToMeAttention</strong>. For an initial sequence $Z_L$, tokens are merged via Bipartite Soft Matching (BSM) to reduce redundancy:
          </p>
          <div class="formula">
            $$S, A_K, Z_K = \text{ToMeAttention}(Z_L, r)$$,
          </div>
          <p>
            Here, $A_K$ represents the condensed attention map, and $S$ is the <strong>Source Matrix</strong>, which tracks the spatial relationship between the raw tokens ($Z_L$) and the merged tokens ($Z_K$).
          </p>
        </div>
    
        <div class="implementation-step">
          <h4 style="color: #2563eb;">Step 2: Generating Mixing Mask with Source Matrix</h4>
          <p>
            To avoid the information loss typical of greedy "Top-K" selection, we introduce a recovery function $\mathcal{R}_{K \to L}$. This mechanism expands the merged attention map back to the original resolution using the source matrix $S$:
          </p>
          <div class="formula">
            $$\hat{A}_L = \mathcal{R}_{K \to L}(A_K, S)$$,
          </div>
          <p>
            This ensures that attention values are propagated over the original token topology, preserving contextual continuity and spatial dependencies.
            The final mixing mask $\mathcal{M}$ is generated by selecting the most significant regions from the recovered attention map $\hat{A}_L$:
          </p>
          <div class="formula">
            $$\mathcal{M}_i = \begin{cases} 1, & \text{if } i \in \text{TopK}(\hat{A}_L, p) \\ 0, & \text{otherwise} \end{cases}$$,
          </div>
          <p>
            where $p = \lfloor\lambda \times L \rfloor$ determines the mixing threshold. This mask is then used to combine image patches from the mini-batch to create augmented training data, $\hat{x}$ = $\mathcal{M} \odot x_i$ + $(1 - \mathcal{M}) \odot x_j$.
          </p>
        </div>

        <div class="implementation-step">
          <h4 style="color: #2563eb;">Step 3: Re-scaling Policy for Mixing Ratio</h4>
          <p>
            Standard mixup methods often rely on simple spatial ratios. In contrast, <strong>MergeMix</strong> introduces an adaptive policy that considers the degree of information integration within the model, utilizing both merged tokens and mask density.
            The refined mixing ratio $\hat{\lambda}$ is sampled from a Gaussian distribution and normalized:
          </p>
            <div class="formula">
              $$\hat{\lambda} \sim \mathcal{N}(\mu, \sigma), \quad \hat{\lambda} = \text{clip} \left( \frac{\hat{\lambda} - \min(\hat{\lambda})}{\max(\hat{\lambda}) - \min(\hat{\lambda}) + \tau}, 0, 1 \right)$$
            </div>
            <p>
              where $\mu = \frac{K}{L}$ is the mean of the Gaussian distribution and $\sigma = \frac{p}{\sum \mathcal{M}}$ is the standard deviation.
            </p>
          <p>
            This Gaussian-based sampling ensures a smooth transition between samples, alleviating the abruptness of linear mapping and resulting in more robust data augmentations.
          </p>
        </div>

        <div class="implementation-step">
          <h4 style="color: #2563eb;">Step 4: Loss Function of Image Classification</h4>
          <p>
            The model is optimized using a composite loss function $\mathcal{L}_{\text{Total}}$, which balances the mixed data supervision with standard one-hot classification:
          </p>  
          <div class="formula">
            $$\mathcal{L}_{\text{Total}} = \underbrace{\mathcal{L}_{\text{CE}}(f_\theta(\hat{x}), y_i) \cdot \hat{\lambda} + \mathcal{L}_{\text{CE}}(f_\theta(\hat{x}), y_j) \cdot (1 - \hat{\lambda})}_{\text{Mixed Content (MCE) Loss}} + \underbrace{\mathcal{L}_{\text{CE}}(f_\theta(x), y)}_{\text{One-Hot Loss}}$$,
          </div>
      
          <p>
            where $\hat{x}$ is the mixed input, $y_i$ and $y_j$ are the source labels, $y$ is the target label, and $\hat{\lambda}$ is the dynamic mixing ratio.
          </p>
        </div>
      </section>

      <section id="results" class="panel fade-up">
        <h2>Results of Image Classification</h2>
        <div class="results-carousel">
          <button class="carousel-btn prev">&#10094;</button>
          <div class="carousel-track">
            <div class="slide active">
              <img src="images/cifar100-results.png" alt="CIFAR100 Results" />
              <p class="caption">CIFAR100 dataset classification (part)</p>
            </div>
            <div class="slide">
              <img src="images/cars-results.png" alt="Stanford-Cars Results" />
              <p class="caption">Stanford-Cars dataset classification</p>
            </div>
            <div class="slide">
              <img src="images/in1k-results.png" alt="ImageNet-1K Results" />
              <p class="caption">ImageNet-1K dataset classification</p>
            </div>
            <div class="slide">
              <img src="images/cifar100-results-full.png" alt="CIFAR100 Results (full)" />
              <p class="caption">CIFAR100 dataset classification (full)</p>
            </div>
            <div class="slide">
              <img src="images/fine-grain-results.png" alt="Fine-Grain Results" />
              <p class="caption">CUB200, FGVC-Aircrafts, and Stanford-Cars datasetes classification</p>
            </div>
            <div class="slide">
              <img src="images/metaformer-results.png" alt="MetaFormer Results" />
              <p class="caption">MetaFormer Results</p>
            </div>
          </div>
          <button class="carousel-btn next">&#10095;</button>
        </div>
      </section>


      <section id="calibration" class="panel fade-up">
        <h2>Results of Calibration</h2>
        <div class="grid">
          <img src="images/calibration.png" alt="Calibration 1" />
          <img src="images/calibration-tab.png" alt="Calibration 2" />
        </div>
      </section>

      <section id="occlusion" class="panel fade-up">
        <h2>Results of Occlusion Classification</h2>
        <div class="results-carousel">
          <button class="carousel-btn prev">&#10094;</button>
          <div class="carousel-track">
            <div class="slide-w active">
              <img src="images/occlusion.png" alt="Occlusion 1" />
              <p class="caption">Curves of occlusion classification results on CIFAR100 dataset</p>
            </div>
            <div class="slide-w active">
              <img src="images/occlusion-tab-1.png" alt="Occlusion 2" />
              <p class="caption">Occlusion Classification Results on CIFAR100 dataset with DeiT-Tiny</p>
            </div>
            <div class="slide-w">
              <img src="images/occlusion-tab-3.png" alt="Occlusion 3" />
              <p class="caption">Occlusion Classification Results on CIFAR100 dataset with ViT-Tiny</p>
            </div>
            <div class="slide-w">
              <img src="images/occlusion-tab-2.png" alt="Occlusion 4" />
              <p class="caption">Occlusion Classification Results on ImageNet-1K dataset with DeiT-Small</p>
            </div>
          </div>
          <button class="carousel-btn next">&#10095;</button>
        </div>
      </section>
    </main>

    <div style="text-align: center; margin: 20px 0;">
      <a class="btn" href="index.html" style="display: inline-block;">Home Page</a>
    </div>

    <footer class="site-footer">
      <div>Â© 2025 Xin Jin. All rights reserved. Part of MergeMix Project.</div>
    </footer>
  </div>

  <script src="js/script.js"></script>
</body>
</html>