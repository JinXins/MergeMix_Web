<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>MergeMix</title>
  <link rel="stylesheet" href="css/style.css" />
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <!-- Academic Icons for arXiv -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <!-- Site logo -->
  <link rel="icon" type="image/png" href="images/westlake.png">
</head>
<body>
  <div class="container">
    <header class="site-header">
      <div class="title-block">
        <h1 class="site-title">MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding</h1>
        <!-- <p class="subtitle">MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding</p> -->
      </div>
      <div class="meta">
        <div class="authors">
          <a href="https://scholar.google.com/citations?user=v3OwxWIAAAAJ&hl=zh-CN" target="_blank">Xin Jin</a><sup>1, *</sup>, 
          <a href="https://scholar.google.com/citations?user=SKTQTXwAAAAJ&hl=zh-CN" target="_blank">Siyuan Li</a><sup>1, 2, *</sup>, 
          <a href="https://scholar.google.com/citations?user=BodnjL0AAAAJ&hl=zh-CN&oi=ao" target="_blank">Siyong Jian</a><sup>1</sup>, 
          <a href="https://openreview.net/profile?id=~Kai_Yu12" target="_blank">Kai Yu</a><sup>1</sup>, 
          <a href="https://scholar.google.com/citations?user=0-On0y4AAAAJ&hl=zh-CN" target="_blank">Huan Wang</a><sup>1, #</sup>
        </div>
        <div class="affiliations">
          1 Westlake University, 
        </div>
        <div class="affiliations">
          2 Zhejiang University, College of Computer Science and Technology
        </div>
        <div class="affiliations">
          <sup>*</sup> denote equal contribution, <sup>#</sup> denotes corresponding author
        </div>
        
        <div class="institution-logos">
          <img src="images/westlake.png" alt="Westlake University" class="inst-logo">
          <img src="images/zju-logo.svg" alt="Zhejiang University" class="inst-logo">
          <img src="images/encode-logo.jpg" alt="EncodeLab" class="inst-logo">
        </div>
        
        <div class="links">
          <a class="btn" href="#">
            <i class="fas fa-file-pdf"></i> Paper (PDF)
          </a>
          <a class="btn" href="#">
            <i class="fab fa-github"></i> Code
          </a>
          <a class="btn" href="#">
            <img src="images/hf-logo.svg" alt="HuggingFace" class="icon-svg" /> HuggingFace
          </a>
          <a class="btn" href="#">
            <i class="ai ai-arxiv"></i> arXiv
          </a>
        </div>
      </div>
    </header>

    <main>
      <section class="teaser fade-up">
        <img src="images/teaser.png" alt="Teaser image" />
        <p class="fig-caption">Efficiency and performance for MergeMix. (a) The training time vs. accuracy of mixup methods with the DeiT-Small model. (b) The image classification Top-1 accuracy vs. training epochs of different mixup methods on the CIFAR100 dataset with the DeiT-Tiny model. (c) The radar plot of the results on part VQA tasks by LLaVA-7B, LLaVA with SFT, and MergeMix.</p>
      </section>

      <section id="abstract" class="panel fade-up">
        <h2>Abstract</h2>
        <p>
          Vision–language alignment in multi-modal large language models (MLLMs) typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). SFT is stable and efficient but requires large-scale human annotations and cannot capture subtle preferences, while RL brings in a reward signal for training, but suffers from overhead and instability. These limitations highlight a trade-off between scalability, robustness, and alignment quality. To address this, we propose MergeMix, a training-time augmentation paradigm that bridges SFT and RL. It first applies an attention-aware image mixing via token merge with more cluster representation and spatial context, and then presents a preference-driven training paradigm for MLLMs by building preference pairs with mixed images and raw images, and optimizing via SimPO loss. As a mixup augmentation, MergeMix enhances attention consistency and efficiency, surpassing other heuristic-based methods in classification. Extensive experiments demonstrate that MergeMix achieves competitive accuracy with improved efficiency, providing a scalable approach to preference alignment in classification and MLLMs.
        </p>
      </section>

      <section id="overview" class="panel fade-up">
        <h2>Method Overview</h2>
        <div class="overview-grid">
          <img src="images/overall.png" alt="Overview figure" />
          <p class="fig-caption">The overall of the two scenarios of MergeMix. (a) MergeMix for Image Classification: The image is processed by the ToMe encoder, with Attention Score Recovery and TopK sampling to generate the corresponding class prediction. (b) MergeMix for MLLM: Preference pairs are encoded by the vision model with token merging, and the LLM decoder generates response text for the loser and winner, optimized via a ranking loss.</p>
          <div>
            <p>Multi-modal Large Language Models (e.g., LLaVA, QwenVL, Cambrain-1, etc.) have recently demonstrated remarkable capabilities in integrating visual and textual information, enabling a wide range of applications from visual question answering to multi-modal reasoning. 
              Since MLLMs are typically pre-trained on massive web-scale datasets, forcing them to possess a wide range of knowledge and general reasoning capabilities, Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)-based preference optimization have emerged as two primary paradigms for aligning MLLMs with human preferences and specific task requirements. 
              However, SFT depends on high-quality instruction–response annotations and optimizes the likelihood of reference responses, which does not explicitly model relative preferences between outputs. 
              RL-based methods such as RLHF are more preference-aware, but they require an additional reward model that may encode bias or be exploited by reward. 
              Researchers have proposed several advance approaches to address these issues. 
              We investigate an interesting question: <strong><i>Is it necessary to propose novel techniques rather than some classical machine learning methods in the MLLM scenario?</i></strong>
            </p>
            <p>
              We revisit the mixup augmentation, which synthesizes mixed samples and corresponding labels with given mixing ratios. However, two main challenges arise:
            </p>
            <ul>
              <li>Achieving an optimal trade-off between efficiency and performance of mixup augmentations that rely on saliency-based metrics.</li>
              <li>Extending the augmentation to MLLMs properly, from classical image corruptions to data-dependent samples.</li>
            </ul>
            <p>
              Motivated by these perspectives, we propose a novel training framework called <b>MergeMix</b>, which builds preference pairs for MLLM training through data augmentation methods and ranking loss, thereby bridging the gap between SFT and RL.
            </p>
            <ul>
              <li>
                <strong>Image Mixing:</strong> 
                A novel data augmentation that generates mixed samples through <u>token merge</u> techniques. A bipartite soft matching (BSM) gathers the similarity information that brings the context, making the mask retain useful features. Meanwhile, MergeMix links the merge ratio and mixing ratio, aligning the information density of samples, enabling precise mixing data generation.
              </li>
              <li>
                <strong>Preference Tuning with Mixup:</strong> 
                A preference-driven SFT paradigm for MLLMs, where augmented samples are regarded as non-preferred responses <u>Loser</u> and clean samples as preferred responses <u>Winner</u>. This enables preference optimization via SimPO loss without relying on reward models.
            </ul>
            <p>
              You could chlick the buttons to see the details of image mixing and preference tuning with mixup.
            </p>
            <div class="method-buttons">
              <a class="btn" href="image-mixing.html">Image Mixing</a>
              <a class="btn" href="preference-tuning.html">Preference Tuning with Mixup</a>
            </div>
          </div>
        </div>
      </section>

      
      <section id="visualization" class="panel fade-up">
        <h2>Visualization of MergeMix</h2>
        <p>
          We provide visualizations of MergeMix on both MLLM case study and mixed images with different mixing ratios.
        </p>
        <h3>MLLM Case Study</h3>
        <div class="grid">
          <img src="images/mllm-case.png" alt="Case Study" />
        </div>
        <h3>Mixed Images</h3>
        <div class="grid">
          <img src="images/mixed-samples.png" alt="MLLM 1" />
        </div>
      </section>

      <!-- BibTeX -->
      <section id="bibtex" class="panel fade-up">
        <h2>BibTeX</h2>
        <div class="bibtex-container">
          <pre class="bibtex">@article{jin2025mergemix,
         title = {MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding},
         author = {Jin, Xin and Li, Siyuan and Jian, Siyong and Yu, Kai and Wang, Huan},
         year = {2025},
         journal = {arXiv preprint arXiv:xxxxx}
}
          </pre>
          <button class="copy-btn" onclick="copyBibtex()">Copy</button>
        </div>
      </section>

    </main>

    <footer class="site-footer">
      <div>© 2025 Xin Jin. All rights reserved. Part of MergeMix Project.</div>
    </footer>
  </div>

  <script src="js/script.js"></script>
</body>
</html>